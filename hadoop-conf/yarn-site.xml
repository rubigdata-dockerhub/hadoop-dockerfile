<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>

  <!-- Site specific YARN configuration properties -->
  <!-- 
       See 
       + https://blog.knoldus.com/install-configure-hadoop-hdfsyarn-cluster-and-integrate-spark-with-it/ 
       + http://bdlabs.edureka.co/static/help/topics/cdh_ig_yarn_cluster_deploy.html
       for more info
  -->
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>redbad02</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>redbad02:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>redbad02:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>redbad02:8031</value>
  </property>
  <property>
    <name>yarn.cluster.max-application-priority</name>
    <value>100</value>
  </property>
  <property>
    <name>yarn.dispatcher.exit-on-error</name>
    <value>true</value>
  </property>
  <property>
    <description>List of directories to store localized files in.
    </description>
    <name>yarn.nodemanager.local-dirs</name>
    <value>/var/storage/data1/yarn/gelre/local,/var/storage/data2/yarn/gelre/local,/var/storage/data3/yarn/gelre/local,/var/storage/data4/yarn/gelre/local,/var/storage/data5/yarn/gelre/local,/var/storage/data6/yarn/gelre/local</value>
  </property>
  <property>
    <description>List of directories to store localized files in.
    </description>
    <name>yarn.nodemanager.log-dirs</name>
    <value>/var/storage/data1/yarn/gelre/log,/var/storage/data2/yarn/gelre/log,/var/storage/data3/yarn/gelre/log,/var/storage/data4/yarn/gelre/log,/var/storage/data5/yarn/gelre/log,/var/storage/data6/yarn/gelre/log</value>
    <!-- <value>/var/storage/data1/yarn/gelre/log</value> -->
  </property>
  <!-- Following: https://stackoverflow.com/questions/32713587/how-to-keep-yarns-log-files -->
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>
  <property>
     <name>yarn.nodemanager.remote-app-log-dir</name>
     <value>/app-logs</value>
  </property>
  <property>
      <name>yarn.nodemanager.remote-app-log-dir-suffix</name>
      <value>logs</value>
  </property>
  <property>
    <name>yarn.log.server.url</name>
    <value>http://rbdata01.cs.ru.nl:19888/jobhistory/logs</value>
  </property>
  <!--
  <property>
      <description>Where to aggregate logs to.</description>
      <name>yarn.nodemanager.remote-app-log-dir</name>
      <value>hdfs://gelre/var/log/hadoop-yarn/apps</value>
    </property>
  -->
  <property>
    <description>Classpath for typical applications.</description>
    <name>yarn.application.classpath</name>
    <value>
      $HADOOP_CONF_DIR,
      /hadoop/*,/hadoop/lib/*,
      $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
      $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,
      $HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*
    </value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>1024</value>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>31744</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>31744</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>5</value>
  </property>
  <property>
    <name>yarn.app.mapreduce.am.resource.memory-mb</name>
    <value>4096</value>
  </property>
  <property>
    <name>yarn.app.mapreduce.am.command-opts</name>
    <value>-Xmx3192m</value>
  </property>
  <property>
    <name>yarn.webapp.ui2.enable</name>
    <value>true</value>
  </property>
  <!-- resource manager secure configuration info -->
  <!-- Secure install will follow...
      <property>
        <name>yarn.resourcemanager.principal</name>
        <value>yarn/_HOST@HADOOP.COM</value>
      </property>
      <property>
        <name>yarn.resourcemanager.keytab</name>
        <value>/home/ubuntu/keytabs/yarn-worker.keytab</value>
      </property>
  -->
  <!-- remember the principal for the node manager is the principal for the host this yarn-site.xml file is on -->
  <!-- these (next four) need only be set on node manager nodes -->
  <!--
      <property>
        <name>yarn.nodemanager.principal</name>
        <value>yarn/_HOST@HADOOP.COM</value>
      </property>
      <property>
        <name>yarn.nodemanager.keytab</name>
        <value>/home/ubuntu/keytabs/yarn-worker.keytab</value>
      </property>
  -->
  <!-- Does this solve some problems?
       <property>
         <name>yarn.nodemanager.container-executor.class</name>
         <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
       </property>
       <property>
         <name>yarn.nodemanager.linux-container-executor.group</name>
         <value>hadoop</value>
       </property>
  -->
  <!-- 
       Instead of the linux-container-executor above;
       can also set this to true and instead give a static username through
       yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user
  -->
  <property>
    <name>yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.resourcemanager.ha.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.resourcemanager.cluster-id</name>
    <value>gelre</value>
  </property>
  <property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
    <value>rm1,rm2</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname.rm1</name>
    <value>redbad02</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname.rm2</name>
    <value>redbad01</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address.rm1</name>
    <value>redbad02:8088</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address.rm2</name>
    <value>redbad01:8088</value>
  </property>
  <property>
    <description>Enable RM to recover state after starting. If true, then
    yarn.resourcemanager.store.class must be specified</description>
    <name>yarn.resourcemanager.recovery.enabled</name>
    <value>true</value>
  </property>
  <property>
    <description>The class to use as the persistent store.</description>
    <name>yarn.resourcemanager.store.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
  </property>
  <property>
    <description>Comma separated list of Host:Port pairs. Each corresponds to a ZooKeeper server
    (e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002") to be used by the RM for storing RM state.
    This must be supplied when using org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore
    as the value for yarn.resourcemanager.store.class</description>
    <name>hadoop.zk.address</name>
    <value>redbad01:2181,redbad02:2181,rbdata01:2181</value>
  </property>
  <!-- Capacity Scheduler -->
    <property>
    <name>yarn.acl.enable</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
  </property>
</configuration>
