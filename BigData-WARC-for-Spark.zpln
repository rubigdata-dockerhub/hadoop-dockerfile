{
  "paragraphs": [
    {
      "text": "%md\n## WARC for Spark\n\nThis notebook will help you get started on developing code to analyze WARC files in Spark inside Spark Notebook.\n\nOnce the code seems ready, you can use _Download as scala_ from the File menu, and continue to develop it into the standalone application to be submitted using `spark-submit`; see specifically the [course instructions on self-contained apps](http://rubigdata.github.io/course/background/sbt.html).",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:32:18+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588608407720_1753054952",
      "id": "paragraph_1588608407720_1753054952",
      "dateCreated": "2020-05-04T16:06:47+0000",
      "dateStarted": "2020-05-04T16:32:18+0000",
      "dateFinished": "2020-05-04T16:32:18+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:4324",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>WARC for Spark</h2>\n<p>This notebook will help you get started on developing code to analyze WARC files in Spark inside Spark Notebook.</p>\n<p>Once the code seems ready, you can use <em>Download as scala</em> from the File menu, and continue to develop it into the standalone application to be submitted using <code>spark-submit</code>; see specifically the <a href=\"http://rubigdata.github.io/course/background/sbt.html\">course instructions on self-contained apps</a>.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "import nl.surfsara.warcutils.WarcInputFormat\nimport org.jwat.warc.{WarcConstants, WarcRecord}\n\nimport org.apache.hadoop.io.LongWritable;\n\nimport org.apache.commons.lang.StringUtils;",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:10:56+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588608310770_596668663",
      "id": "paragraph_1588608310770_596668663",
      "dateCreated": "2020-05-04T16:05:10+0000",
      "dateStarted": "2020-05-04T16:05:12+0000",
      "dateFinished": "2020-05-04T16:05:20+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4325"
    },
    {
      "text": "%md\nUsing Spark, classes may need to be shipped between different nodes in the cluster, which involves their serialization.\n\nIf you get errors that classes are not serializable, add those to the .set commands below - which involves a reset of the notebook, so existing variables will not exist any more.",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:33:23+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588608865290_940510838",
      "id": "paragraph_1588608865290_940510838",
      "dateCreated": "2020-05-04T16:14:25+0000",
      "dateStarted": "2020-05-04T16:33:23+0000",
      "dateFinished": "2020-05-04T16:33:23+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4326",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Using Spark, classes may need to be shipped between different nodes in the cluster, which involves their serialization.</p>\n<p>If you get errors that classes are not serializable, add those to the .set commands below - which involves a reset of the notebook, so existing variables will not exist any more.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "// Adapt the SparkConf to use Kryo and register the classes used through reset parameter lastChanges (a function)\n//import org.apache.spark.{Logging, SparkConf}\nimport org.apache.spark.SparkConf\n\n// reset( lastChanges= _.\n//       set( \"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\" ).\n//       set( \"spark.kryo.classesToRegister\", \n//           \"org.apache.hadoop.io.LongWritable,\" +\n//           \"org.jwat.warc.WarcRecord,\" +\n//           \"org.jwat.warc.WarcHeader\" )\n//       )",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:17:26+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588608908755_-786745145",
      "id": "paragraph_1588608908755_-786745145",
      "dateCreated": "2020-05-04T16:15:08+0000",
      "dateStarted": "2020-05-04T16:17:26+0000",
      "dateFinished": "2020-05-04T16:17:26+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4327"
    },
    {
      "text": "%md\n### Use WARC contents\nLet us load some WARC file and carry out a few analyses.",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:33:43+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609088706_1169647768",
      "id": "paragraph_1588609088706_1169647768",
      "dateCreated": "2020-05-04T16:18:08+0000",
      "dateStarted": "2020-05-04T16:33:43+0000",
      "dateFinished": "2020-05-04T16:33:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4328",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Use WARC contents</h3>\n<p>Let us load some WARC file and carry out a few analyses.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%md\nAssign `warcfile` an example WARC file to work with; I used `wget` to create a WARC file from the course website, see e.g. [the final assignment](http://rubigdata.github.io/course/assignments/P-commoncrawl.html).",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:33:45+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609420701_-1754902728",
      "id": "paragraph_1588609420701_-1754902728",
      "dateCreated": "2020-05-04T16:23:40+0000",
      "dateStarted": "2020-05-04T16:33:45+0000",
      "dateFinished": "2020-05-04T16:33:45+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4329",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Assign <code>warcfile</code> an example WARC file to work with; I used <code>wget</code> to create a WARC file from the course website, see e.g. <a href=\"http://rubigdata.github.io/course/assignments/P-commoncrawl.html\">the final assignment</a>.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "val warcfile = \"/root/bigdata/course.warc.gz\"",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:21:28+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609132466_2001529399",
      "id": "paragraph_1588609132466_2001529399",
      "dateCreated": "2020-05-04T16:18:52+0000",
      "dateStarted": "2020-05-04T16:21:28+0000",
      "dateFinished": "2020-05-04T16:21:28+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4330"
    },
    {
      "text": "%md\nNow initialize an RDD from warcfile using the WarcInputFormat parser provided by nl.surfsara.warcutils:",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:33:47+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609296420_276145769",
      "id": "paragraph_1588609296420_276145769",
      "dateCreated": "2020-05-04T16:21:36+0000",
      "dateStarted": "2020-05-04T16:33:47+0000",
      "dateFinished": "2020-05-04T16:33:47+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4331",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Now initialize an RDD from warcfile using the WarcInputFormat parser provided by nl.surfsara.warcutils:</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "val warcf = sc.newAPIHadoopFile(\n              warcfile,\n              classOf[WarcInputFormat],               // InputFormat\n              classOf[LongWritable],                  // Key\n              classOf[WarcRecord]                     // Value\n    )",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:22:02+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609307673_-2016189689",
      "id": "paragraph_1588609307673_-2016189689",
      "dateCreated": "2020-05-04T16:21:47+0000",
      "dateStarted": "2020-05-04T16:22:02+0000",
      "dateFinished": "2020-05-04T16:22:03+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4332"
    },
    {
      "text": "%md\n__Note:__ My initial approach was to cache the constructed RDD; unfortunately, doing this interacts somehow (I do not exactly understand why yet) with the inner workings of the WarcRecord classes, resulting in java.io.IOException: Stream closed errors when operating on the payloads in these WarcRecords.\n\nI resorted to defining warc as a cached version, after an identity transform, and warcc as a transformation of warcf that already extracted the contents. In your own code, try to ensure that you filter the stream as much as possible before accessing the contents though (so do not build on the warcc result if you are not using all records).",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:33:50+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609349275_1871929838",
      "id": "paragraph_1588609349275_1871929838",
      "dateCreated": "2020-05-04T16:22:29+0000",
      "dateStarted": "2020-05-04T16:33:50+0000",
      "dateFinished": "2020-05-04T16:33:50+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4333",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><strong>Note:</strong> My initial approach was to cache the constructed RDD; unfortunately, doing this interacts somehow (I do not exactly understand why yet) with the inner workings of the WarcRecord classes, resulting in java.io.IOException: Stream closed errors when operating on the payloads in these WarcRecords.</p>\n<p>I resorted to defining warc as a cached version, after an identity transform, and warcc as a transformation of warcf that already extracted the contents. In your own code, try to ensure that you filter the stream as much as possible before accessing the contents though (so do not build on the warcc result if you are not using all records).</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%md\n#### Using header info only",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:33:52+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609380037_-2104159242",
      "id": "paragraph_1588609380037_-2104159242",
      "dateCreated": "2020-05-04T16:23:00+0000",
      "dateStarted": "2020-05-04T16:33:52+0000",
      "dateFinished": "2020-05-04T16:33:52+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4334",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h4>Using header info only</h4>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "val warc = warcf.map{wr => wr}.cache()",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:24:20+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609452429_-897535356",
      "id": "paragraph_1588609452429_-897535356",
      "dateCreated": "2020-05-04T16:24:12+0000",
      "dateStarted": "2020-05-04T16:24:20+0000",
      "dateFinished": "2020-05-04T16:24:21+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4335"
    },
    {
      "text": "val nHTML = warc.count()",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:24:29+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://1c9cac19d8ef:4040/jobs/job?id=0",
              "$$hashKey": "object:5391"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609468093_-1569481143",
      "id": "paragraph_1588609468093_-1569481143",
      "dateCreated": "2020-05-04T16:24:28+0000",
      "dateStarted": "2020-05-04T16:24:29+0000",
      "dateFinished": "2020-05-04T16:24:31+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4336"
    },
    {
      "text": "// WarcRecords header type info\nwarc.map{ wr => wr._2.header }.\nmap{ h => (h.warcTypeIdx, h.warcTypeStr) }.take(10)",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:26:39+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://1c9cac19d8ef:4040/jobs/job?id=4",
              "$$hashKey": "object:5439"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609479422_-2023549617",
      "id": "paragraph_1588609479422_-2023549617",
      "dateCreated": "2020-05-04T16:24:39+0000",
      "dateStarted": "2020-05-04T16:26:39+0000",
      "dateFinished": "2020-05-04T16:26:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4337"
    },
    {
      "text": "// Get responses with their size\nwarc.map{ wr => wr._2.header }.\n     filter{ _.warcTypeIdx == 2 /* response */ }.\n     map{ h => (h.warcTargetUriStr, h.contentLength, h.contentType.toString) }.collect().\n     foreach { println }",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:28:15+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 84,
              "optionOpen": false
            }
          }
        },
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://1c9cac19d8ef:4040/jobs/job?id=11",
              "$$hashKey": "object:5487"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609488058_-646296040",
      "id": "paragraph_1588609488058_-646296040",
      "dateCreated": "2020-05-04T16:24:48+0000",
      "dateStarted": "2020-05-04T16:28:15+0000",
      "dateFinished": "2020-05-04T16:28:16+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4338"
    },
    {
      "text": "// WarcRecords with responses that gave a 404:\nwarc.map{ wr => wr._2 }.\n     filter{ _.header.warcTypeIdx == 2 /* response */ }.\n     filter{ _.getHttpHeader().statusCode == 404 }.\n     map{ wr => wr.header.warcTargetUriStr }. collect().\n     foreach { println }",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:28:12+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://1c9cac19d8ef:4040/jobs/job?id=10",
              "$$hashKey": "object:5535"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609507230_-1296478454",
      "id": "paragraph_1588609507230_-1296478454",
      "dateCreated": "2020-05-04T16:25:07+0000",
      "dateStarted": "2020-05-04T16:28:12+0000",
      "dateFinished": "2020-05-04T16:28:12+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4339"
    },
    {
      "text": "// WarcRecords corresponding to HTML responses:\nwarc.map{ wr => wr._2 }.\n     filter{ _.header.warcTypeIdx == 2 /* response */ }.\n     filter{ _.getHttpHeader().contentType.startsWith(\"text/html\") }.\n     map{ wr => (wr.header.warcTargetUriStr, wr.getHttpHeader().contentType) }. collect()\n     .foreach{ println }",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:29:27+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://1c9cac19d8ef:4040/jobs/job?id=13",
              "$$hashKey": "object:5583"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609711163_2115733464",
      "id": "paragraph_1588609711163_2115733464",
      "dateCreated": "2020-05-04T16:28:31+0000",
      "dateStarted": "2020-05-04T16:29:27+0000",
      "dateFinished": "2020-05-04T16:29:27+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4340"
    },
    {
      "text": "%md\n#### Using contents\nDefine a utility function to get access to the Payload, i.e., the actual contents of the WarcRecords.",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:32:11+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true,
        "title": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609791682_1255723765",
      "id": "paragraph_1588609791682_1255723765",
      "dateCreated": "2020-05-04T16:29:51+0000",
      "dateStarted": "2020-05-04T16:32:11+0000",
      "dateFinished": "2020-05-04T16:32:11+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4341",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h4>Using contents</h4>\n<p>Define a utility function to get access to the Payload, i.e., the actual contents of the WarcRecords.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "import java.io.InputStreamReader;\ndef getContent(record: WarcRecord):String = {\n  val cLen = record.header.contentLength.toInt\n  //val cStream = record.getPayload.getInputStreamComplete()\n  val cStream = record.getPayload.getInputStream()\n  val content = new java.io.ByteArrayOutputStream();\n\n  val buf = new Array[Byte](cLen)\n  \n  var nRead = cStream.read(buf)\n  while (nRead != -1) {\n    content.write(buf, 0, nRead)\n    nRead = cStream.read(buf)\n  }\n\n  cStream.close()\n  \n  content.toString(\"UTF-8\");\n}",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:30:16+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609802622_-166858166",
      "id": "paragraph_1588609802622_-166858166",
      "dateCreated": "2020-05-04T16:30:02+0000",
      "dateStarted": "2020-05-04T16:30:16+0000",
      "dateFinished": "2020-05-04T16:30:16+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4342"
    },
    {
      "text": "// Taking a substring to avoid messing up the rendering of results in the Notebook - would need proper handling\nval warcc = warcf.\n  filter{ _._2.header.warcTypeIdx == 2 /* response */ }.\n  filter{ _._2.getHttpHeader().contentType.startsWith(\"text/html\") }.\n  map{wr => (wr._2.header.warcTargetUriStr, StringUtils.substring(getContent(wr._2), 0, 256))}.cache()",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:30:32+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609826745_-1812427005",
      "id": "paragraph_1588609826745_-1812427005",
      "dateCreated": "2020-05-04T16:30:26+0000",
      "dateStarted": "2020-05-04T16:30:32+0000",
      "dateFinished": "2020-05-04T16:30:32+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4343"
    },
    {
      "text": "warcc.take(10)",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:30:41+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://1c9cac19d8ef:4040/jobs/job?id=14",
              "$$hashKey": "object:5755"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588609839577_-369879590",
      "id": "paragraph_1588609839577_-369879590",
      "dateCreated": "2020-05-04T16:30:39+0000",
      "dateStarted": "2020-05-04T16:30:41+0000",
      "dateFinished": "2020-05-04T16:30:41+0000",
      "status": "FINISHED",
      "$$hashKey": "object:4344"
    },
    {
      "user": "anonymous",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588610038807_1350393434",
      "id": "paragraph_1588610038807_1350393434",
      "dateCreated": "2020-05-04T16:33:58+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:6376",
      "text": "%md\n### Example: Use Jsoup to convert HTML to Text\n\n[Jsoup](https://jsoup.org/) is a widely used library to clean HTML.",
      "dateUpdated": "2020-05-04T16:49:58+0000",
      "dateFinished": "2020-05-04T16:34:52+0000",
      "dateStarted": "2020-05-04T16:34:52+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Example: Use Jsoup to convert HTML to Text</h3>\n<p><a href=\"https://jsoup.org/\">Jsoup</a> is a widely used library to clean HTML.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "import java.io.IOException;\nimport org.jsoup.Jsoup;\ndef HTML2Txt(content: String) = {\n  try {\n    Jsoup.parse(content).text().replaceAll(\"[\\\\r\\\\n]+\", \" \")\n  }\n  catch {\n    case e: Exception => throw new IOException(\"Caught exception processing input row \", e)\n  }\n}",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:49:53+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588610097859_1707259655",
      "id": "paragraph_1588610097859_1707259655",
      "dateCreated": "2020-05-04T16:34:57+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:6461",
      "dateFinished": "2020-05-04T16:35:22+0000",
      "dateStarted": "2020-05-04T16:35:21+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import java.io.IOException\nimport org.jsoup.Jsoup\n\u001b[1m\u001b[34mHTML2Txt\u001b[0m: \u001b[1m\u001b[32m(content: String)String\u001b[0m\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588610124502_1888195044",
      "id": "paragraph_1588610124502_1888195044",
      "dateCreated": "2020-05-04T16:35:24+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:6555",
      "text": "val warcc = warcf.\n  filter{ _._2.header.warcTypeIdx == 2 /* response */ }.\n  filter{ _._2.getHttpHeader().contentType.startsWith(\"text/html\") }.\n  map{wr => ( wr._2.header.warcTargetUriStr, HTML2Txt(getContent(wr._2)) )}.cache()",
      "dateUpdated": "2020-05-04T16:49:50+0000",
      "dateFinished": "2020-05-04T16:35:33+0000",
      "dateStarted": "2020-05-04T16:35:33+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mwarcc\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, String)]\u001b[0m = MapPartitionsRDD[48] at map at <console>:40\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "_1": "string",
                      "_2": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://1c9cac19d8ef:4040/jobs/job?id=25",
              "$$hashKey": "object:9340"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588610142609_-706782000",
      "id": "paragraph_1588610142609_-706782000",
      "dateCreated": "2020-05-04T16:35:42+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:6649",
      "text": "warcc.map{ tt => (tt._1, StringUtils.substring(tt._2, 0, 128))}.take(10).foreach{ println }",
      "dateUpdated": "2020-05-04T16:49:47+0000",
      "dateFinished": "2020-05-04T16:48:14+0000",
      "dateStarted": "2020-05-04T16:48:14+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(http://rubigdata.github.io/course/,301 Moved Permanently 301 Moved Permanently nginx)\n(https://rubigdata.github.io/course/,Course Information RU Big Data 2019 (NWI-IBC036) Course Information RU Big Data 2020 (NWI-IBC036) Related Course Information The)\n(https://rubigdata.github.io/robots.txt,Page not found · GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If thi)\n(https://rubigdata.github.io/coursenil,Page not found · GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If thi)\n(https://rubigdata.github.io/course,301 Moved Permanently 301 Moved Permanently nginx)\n(https://rubigdata.github.io/course/,Course Information RU Big Data 2019 (NWI-IBC036) Course Information RU Big Data 2020 (NWI-IBC036) Related Course Information The)\n(https://rubigdata.github.io/course/assignments/A1a-blogging.html,Assignment 1a RU Big Data 2019 (NWI-IBC036) Assignment 1a Blogging environment Content over layout Grading this course will not )\n(https://rubigdata.github.io/course/assignments/A1b-docker.html,Assignment 1B RU Big Data 2019 (NWI-IBC036) Assignment 1B Docker Docker Environment We use Docker to reduce the burden of unifyi)\n(https://rubigdata.github.io/course/assignments/A2-mapreduce.html,Map Reduce RU Big Data 2019 (NWI-IBC036) Map Reduce Hands-on session with Hadoop 2.9.2 and Map-Reduce Hadoop is an open source p)\n(https://rubigdata.github.io/course/assignments/A3-spark.html,Spark RU Big Data 2019 (NWI-IBC036) Spark Hands-on session with Spark RDDs, SQL and DataFrames Assignment 3 consists of two part)\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588610970374_875147601",
      "id": "paragraph_1588610970374_875147601",
      "dateCreated": "2020-05-04T16:49:30+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:9173",
      "text": "%md\n### Final words\n\nNow it is time to continue to develop your own project.\n\nDo not worry about a _\\\"required\\\"_ level of success; it does not have to be a publishable study!\nIt is perfectly fine if you only realize no more than rather simple standalone program that executes on the cluster\nbut does not run on the complete crawl, or uses only header information. \n\n**Even simple tasks are challenging when carried out on large data!**\n\nDo not be too ambitious, and make progress step by step.\n\nThe examples presented in this notebook are meant to be helpful, but they are by no means complete and have not been tested thoroughly on actual data.\nYou may encounter weird problems, complex enough such that there exists no immediate answer on StackExchange.\n\nI hope the course provided enough background on Spark to spot what the cause of the problem might be;\nhowever, if you spend more than say two to three hours on analyzing \nand debugging a challenge, I recommend to give up and modify your objective - consider a different (simpler) project and only scale up later on\n(provided there is still time left).\n\n_If you cannot solve a problem, definitely do call out by creating a new issue on the Forum - maybe one of us knows the answer already!_",
      "dateUpdated": "2020-05-04T16:49:40+0000",
      "dateFinished": "2020-05-04T16:49:35+0000",
      "dateStarted": "2020-05-04T16:49:35+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Final words</h3>\n<p>Now it is time to continue to develop your own project.</p>\n<p>Do not worry about a <em>&quot;required&quot;</em> level of success; it does not have to be a publishable study!<br />\nIt is perfectly fine if you only realize no more than rather simple standalone program that executes on the cluster<br />\nbut does not run on the complete crawl, or uses only header information.</p>\n<p><strong>Even simple tasks are challenging when carried out on large data!</strong></p>\n<p>Do not be too ambitious, and make progress step by step.</p>\n<p>The examples presented in this notebook are meant to be helpful, but they are by no means complete and have not been tested thoroughly on actual data.<br />\nYou may encounter weird problems, complex enough such that there exists no immediate answer on StackExchange.</p>\n<p>I hope the course provided enough background on Spark to spot what the cause of the problem might be;<br />\nhowever, if you spend more than say two to three hours on analyzing<br />\nand debugging a challenge, I recommend to give up and modify your objective - consider a different (simpler) project and only scale up later on<br />\n(provided there is still time left).</p>\n<p><em>If you cannot solve a problem, definitely do call out by creating a new issue on the Forum - maybe one of us knows the answer already!</em></p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "2020-05-04T16:49:44+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1588610975717_-1997699569",
      "id": "paragraph_1588610975717_-1997699569",
      "dateCreated": "2020-05-04T16:49:35+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:9242"
    }
  ],
  "name": "tes",
  "id": "2F7Q9BRHE",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-preview1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/tes"
}